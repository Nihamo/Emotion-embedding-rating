{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-U8ZQANhiiT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# ðŸ”¹ Load datasets\n",
        "train_df = pd.read_csv(\"/content/train.csv\")\n",
        "test_df = pd.read_csv(\"/content/test.csv\")\n",
        "\n",
        "# ðŸ”¹ Extract features and target\n",
        "X = train_df.drop(columns=[\"rating\"])  # Assuming \"rating\" is the target column\n",
        "y = train_df[\"rating\"]\n",
        "\n",
        "# ðŸ”¹ Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ðŸ”¹ Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(test_df.drop(columns=[\"id\"]))  # Drop \"id\" from test data\n",
        "\n",
        "# ðŸ”¹ Train XGBoost model\n",
        "xgb_model = xgb.XGBRegressor(n_estimators=200, learning_rate=0.05, random_state=42)\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# ðŸ”¹ Predict on validation set\n",
        "y_pred = xgb_model.predict(X_val)\n",
        "\n",
        "# ðŸ”¹ Compute evaluation metrics\n",
        "mse = mean_squared_error(y_val, y_pred)\n",
        "y_pred_rounded = np.round(y_pred)  # Convert predictions to nearest integer\n",
        "accuracy = accuracy_score(y_val, y_pred_rounded)\n",
        "\n",
        "# ðŸ”¹ Compute Harmonic Score\n",
        "hs = 6 * ((1/mse) * accuracy) / ((1/mse) + accuracy)\n",
        "\n",
        "print(f\"MSE: {mse}\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Harmonic Score: {hs}\")\n",
        "\n",
        "# ðŸ”¹ Predict on test set\n",
        "test_preds = xgb_model.predict(X_test_scaled)\n",
        "test_preds_rounded = np.round(test_preds)  # Ensure it's in correct format\n",
        "\n",
        "# ðŸ”¹ Create submission.csv\n",
        "submission = pd.DataFrame({\"id\": test_df[\"id\"], \"score\": test_preds_rounded})\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Submission file saved as submission.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.columns)\n"
      ],
      "metadata": {
        "id": "vz_6L2qGjOL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "import xgboost as xgb\n",
        "\n",
        "# ðŸ”¹ Load the dataset\n",
        "train_df = pd.read_csv(\"/content/train.csv\")\n",
        "test_df = pd.read_csv(\"/content/test.csv\")\n",
        "\n",
        "# ðŸ”¹ Extract features and target from train data\n",
        "X = train_df.drop(columns=[\"ID\", \"score\"])  # âœ… Drop 'ID' column to match test set\n",
        "y = train_df[\"score\"]\n",
        "\n",
        "# ðŸ”¹ Split into train-validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ðŸ”¹ Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# ðŸ”¹ Fix test data: Drop 'ID' before scaling\n",
        "X_test_scaled = scaler.transform(test_df.drop(columns=[\"ID\"]))  # âœ… Now matches training features\n",
        "\n",
        "# ðŸ”¹ Train KNN Model\n",
        "knn_model = KNeighborsRegressor(n_neighbors=10, weights=\"distance\")  # Weighted KNN\n",
        "knn_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# ðŸ”¹ Predict using KNN\n",
        "y_pred_knn_train = knn_model.predict(X_train_scaled)\n",
        "y_pred_knn_test = knn_model.predict(X_test_scaled)\n",
        "\n",
        "# ðŸ”¹ Train XGBoost Model\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train_scaled, y_pred_knn_train)  # ðŸ”¹ XGBoost trained on KNN predictions\n",
        "\n",
        "# ðŸ”¹ Predict using XGBoost\n",
        "y_pred_xgb_test = xgb_model.predict(X_test_scaled)\n",
        "\n",
        "# ðŸ”¹ Create submission file\n",
        "submission = pd.DataFrame({\"ID\": test_df[\"ID\"], \"score\": y_pred_xgb_test})\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Hybrid KNN + XGBoost model trained and submission.csv created successfully!\")\n"
      ],
      "metadata": {
        "id": "6sZrUnoQmQi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ðŸ”¹ Load the submission file\n",
        "submission = pd.read_csv(\"submission.csv\")\n",
        "\n",
        "# ðŸ”¹ Round the 'score' column to the nearest integer\n",
        "submission[\"score\"] = submission[\"score\"].round().astype(int)\n",
        "\n",
        "# ðŸ”¹ Save it back to the same file\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Rounded scores (integers) updated in submission.csv successfully!\")\n"
      ],
      "metadata": {
        "id": "72N8WHSqsqaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, LSTM, Flatten, Dense, BatchNormalization, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Load Data\n",
        "train_data = pd.read_csv(\"/content/train.csv\")\n",
        "\n",
        "# Feature Engineering (Selecting Important Features)\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "X_raw = train_data.drop(columns=[\"ID\", \"score\"])\n",
        "y = train_data[\"score\"]\n",
        "\n",
        "mi_scores = mutual_info_regression(X_raw, y)\n",
        "important_features = X_raw.columns[np.argsort(mi_scores)[-35:]]  # Top 35 features\n",
        "\n",
        "X = train_data[important_features]\n",
        "\n",
        "# Train-Val Split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "\n",
        "# Normalize Features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Reshape for CNN + LSTM\n",
        "X_train_cnn = X_train_scaled.reshape(-1, X_train.shape[1], 1)\n",
        "X_val_cnn = X_val_scaled.reshape(-1, X_train.shape[1], 1)\n",
        "\n",
        "# âœ… Optimized CNN + LSTM Model\n",
        "cnn_lstm_model = Sequential([\n",
        "    Conv1D(filters=192, kernel_size=3, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001), input_shape=(X_train.shape[1], 1)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    LSTM(192, return_sequences=True),\n",
        "    LSTM(96, return_sequences=True),\n",
        "    LSTM(48),\n",
        "    Dense(192, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    Dropout(0.2),\n",
        "    Dense(96, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "cnn_lstm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), loss=\"mse\")\n",
        "\n",
        "# Train CNN + LSTM with Early Stopping\n",
        "cnn_lstm_model.fit(X_train_cnn, y_train, validation_data=(X_val_cnn, y_val),\n",
        "                   epochs=80, batch_size=32, verbose=1,\n",
        "                   callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)])\n",
        "\n",
        "# Predict using CNN + LSTM\n",
        "cnn_lstm_preds_train = cnn_lstm_model.predict(X_train_cnn).flatten()\n",
        "cnn_lstm_preds_val = cnn_lstm_model.predict(X_val_cnn).flatten()\n",
        "\n",
        "# âœ… Optimized XGBoost Model\n",
        "# âœ… Optimized XGBoost Model\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    n_estimators=1500,\n",
        "    learning_rate=0.02,\n",
        "    max_depth=10,\n",
        "    min_child_weight=1,\n",
        "    colsample_bytree=0.9,\n",
        "    subsample=0.95,\n",
        "    reg_lambda=1.5,\n",
        "    reg_alpha=1,\n",
        "    objective=\"reg:squarederror\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# âœ… Train XGBoost Model (without early stopping)\n",
        "xgb_model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    eval_set=[(X_val_scaled, y_val)],\n",
        "    eval_metric=\"rmse\",  # âœ… Avoids eval metric error\n",
        "    verbose=True  # âœ… No more early stopping errors\n",
        ")\n",
        "\n",
        "\n",
        "# Predict using XGBoost\n",
        "xgb_preds_train = xgb_model.predict(X_train_scaled)\n",
        "xgb_preds_val = xgb_model.predict(X_val_scaled)\n",
        "\n",
        "# âœ… Improved Stacking Model (Using RandomForest)\n",
        "stacked_train = np.column_stack((cnn_lstm_preds_train, xgb_preds_train))\n",
        "stacked_val = np.column_stack((cnn_lstm_preds_val, xgb_preds_val))\n",
        "\n",
        "meta_model = RandomForestRegressor(n_estimators=500, max_depth=8, random_state=42)\n",
        "meta_model.fit(stacked_train, y_train)\n",
        "\n",
        "# Final Prediction\n",
        "final_preds = meta_model.predict(stacked_val)\n",
        "\n",
        "# âœ… **Correct Evaluation Metrics**\n",
        "n = len(y_val)\n",
        "mse = mean_squared_error(y_val, final_preds)\n",
        "accuracy = np.mean(np.round(final_preds) == np.round(y_val))\n",
        "\n",
        "# âœ… Improved Harmonic Score Calculation\n",
        "hs = (6 * (1/mse) * accuracy) / ((1/mse) + accuracy)\n",
        "\n",
        "print(f\"ðŸ”¹ Optimized MSE: {mse:.4f}, Accuracy: {accuracy:.4f}, HM Score: {hs:.4f}\")\n",
        "\n",
        "# ðŸ”¹ðŸ”¹ðŸ”¹ TEST SUBMISSION ðŸ”¹ðŸ”¹ðŸ”¹\n",
        "\n",
        "# Load Test Data\n",
        "test_data = pd.read_csv(\"/content/test.csv\")\n",
        "\n",
        "# Prepare Test Features\n",
        "X_test = test_data[important_features]\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "X_test_cnn = X_test_scaled.reshape(-1, X_test.shape[1], 1)\n",
        "\n",
        "# Predict using CNN + LSTM and XGBoost\n",
        "cnn_lstm_preds_test = cnn_lstm_model.predict(X_test_cnn).flatten()\n",
        "xgb_preds_test = xgb_model.predict(X_test_scaled)\n",
        "\n",
        "# Stacked Predictions for Test Set\n",
        "stacked_test = np.column_stack((cnn_lstm_preds_test, xgb_preds_test))\n",
        "\n",
        "# Meta-Model Final Prediction\n",
        "final_test_preds = meta_model.predict(stacked_test)\n",
        "\n",
        "# âœ… Round Predictions and Save Submission\n",
        "submission = pd.DataFrame({\"ID\": test_data[\"ID\"], \"score\": np.round(final_test_preds).astype(int)})\n",
        "submission.to_csv(\"submission_optimized.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Improved submission.csv created with optimized predictions ðŸŽ¯\")\n"
      ],
      "metadata": {
        "id": "HvGUkXlo2RAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, LSTM, Bidirectional, Flatten, Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load Data\n",
        "train_data = pd.read_csv(\"/content/train.csv\")\n",
        "\n",
        "# Features and Target\n",
        "X = train_data.drop(columns=[\"ID\", \"score\"])\n",
        "y = train_data[\"score\"]\n",
        "\n",
        "# Handle NaNs\n",
        "X.fillna(X.mean(), inplace=True)\n",
        "y.fillna(y.median(), inplace=True)\n",
        "\n",
        "# Train-Val Split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize Features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Reshape for CNN + LSTM\n",
        "X_train_cnn = X_train_scaled.reshape(-1, X_train.shape[1], 1)\n",
        "X_val_cnn = X_val_scaled.reshape(-1, X_train.shape[1], 1)\n",
        "\n",
        "# âœ… Improved CNN + Bidirectional LSTM Model\n",
        "cnn_lstm_model = Sequential([\n",
        "    Conv1D(filters=256, kernel_size=3, activation='relu', kernel_regularizer=l2(0.01), input_shape=(X_train.shape[1], 1)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.4),\n",
        "    Bidirectional(LSTM(128, return_sequences=True)),\n",
        "    Dropout(0.4),\n",
        "    Bidirectional(LSTM(64)),\n",
        "    Dense(256, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(1)  # Regression output\n",
        "])\n",
        "\n",
        "# âœ… Fine-Tuned Learning Rate\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "cnn_lstm_model.compile(optimizer=optimizer, loss=\"mse\")\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=5, min_lr=1e-6),\n",
        "    EarlyStopping(monitor=\"val_loss\", patience=12, restore_best_weights=True)\n",
        "]\n",
        "\n",
        "# Train CNN + LSTM\n",
        "cnn_lstm_model.fit(X_train_cnn, y_train, validation_data=(X_val_cnn, y_val),\n",
        "                   epochs=80, batch_size=64, verbose=1, callbacks=callbacks)\n",
        "\n",
        "# Predictions\n",
        "cnn_lstm_preds_train = cnn_lstm_model.predict(X_train_cnn).flatten()\n",
        "cnn_lstm_preds_val = cnn_lstm_model.predict(X_val_cnn).flatten()\n",
        "\n",
        "# âœ… Optimized XGBoost Model\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.02,\n",
        "    max_depth=12,\n",
        "    min_child_weight=2,\n",
        "    colsample_bytree=0.9,\n",
        "    subsample=0.9,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=0.5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# XGBoost Predictions\n",
        "xgb_preds_train = xgb_model.predict(X_train_scaled)\n",
        "xgb_preds_val = xgb_model.predict(X_val_scaled)\n",
        "\n",
        "# âœ… LightGBM Meta-Model\n",
        "meta_model = lgb.LGBMRegressor(\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.01,\n",
        "    max_depth=6,\n",
        "    num_leaves=30,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=0.2,\n",
        "    reg_lambda=0.3\n",
        ")\n",
        "\n",
        "# Stacking: CNN+LSTM + XGBoost\n",
        "stacked_train = np.column_stack((cnn_lstm_preds_train, xgb_preds_train))\n",
        "stacked_val = np.column_stack((cnn_lstm_preds_val, xgb_preds_val))\n",
        "\n",
        "# Train Meta-Model\n",
        "meta_model.fit(stacked_train, y_train)\n",
        "\n",
        "# Final Predictions\n",
        "final_preds = meta_model.predict(stacked_val)\n",
        "\n",
        "# âœ… Weighted Ensemble (Stacked + Averaging)\n",
        "final_preds = 0.6 * final_preds + 0.2 * cnn_lstm_preds_val + 0.2 * xgb_preds_val\n",
        "\n",
        "# âœ… Fixed Evaluation Metrics\n",
        "mse = mean_squared_error(y_val, final_preds)\n",
        "exact_matches = np.sum(np.round(final_preds) == np.round(y_val))\n",
        "accuracy = exact_matches / len(y_val)\n",
        "\n",
        "# âœ… Harmonic Score Formula\n",
        "hs = (6 * (1/mse) * accuracy) / ((1/mse) + accuracy)\n",
        "\n",
        "print(f\"ðŸ”¹ MSE: {mse:.4f}, Accuracy: {accuracy:.4f}, Harmonic Score: {hs:.4f}\")\n",
        "\n",
        "# ðŸ”¹ðŸ”¹ðŸ”¹ TEST SUBMISSION ðŸ”¹ðŸ”¹ðŸ”¹\n",
        "test_data = pd.read_csv(\"/content/test.csv\")\n",
        "\n",
        "# Extract ID\n",
        "test_ids = test_data[\"ID\"]\n",
        "\n",
        "# Prepare Test Features\n",
        "X_test = test_data.drop(columns=[\"ID\"])\n",
        "X_test.fillna(X_test.mean(), inplace=True)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "X_test_cnn = X_test_scaled.reshape(-1, X_test.shape[1], 1)\n",
        "\n",
        "# Predict using CNN + LSTM and XGBoost\n",
        "cnn_lstm_preds_test = cnn_lstm_model.predict(X_test_cnn).flatten()\n",
        "xgb_preds_test = xgb_model.predict(X_test_scaled)\n",
        "\n",
        "# Stacked Predictions for Test Set\n",
        "stacked_test = np.column_stack((cnn_lstm_preds_test, xgb_preds_test))\n",
        "\n",
        "# Meta-Model Final Prediction\n",
        "final_test_preds = meta_model.predict(stacked_test)\n",
        "\n",
        "# Weighted Averaging for Final Test Predictions\n",
        "final_test_preds = 0.6 * final_test_preds + 0.2 * cnn_lstm_preds_test + 0.2 * xgb_preds_test\n",
        "\n",
        "# Create Submission File\n",
        "submission = pd.DataFrame({\"ID\": test_ids, \"score\": final_test_preds})\n",
        "submission.to_csv(\"submission_improved.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Submission file saved as submission_improved.csv ðŸŽ¯\")\n"
      ],
      "metadata": {
        "id": "coRqC_5dX-Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, LSTM, Bidirectional, Flatten, Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load Data\n",
        "train_data = pd.read_csv(\"/content/train.csv\")\n",
        "\n",
        "# Features and Target\n",
        "X = train_data.drop(columns=[\"ID\", \"score\"])\n",
        "y = train_data[\"score\"]\n",
        "\n",
        "# Handle NaNs More Effectively\n",
        "X.fillna(X.median(), inplace=True)\n",
        "y.fillna(y.median(), inplace=True)\n",
        "\n",
        "# Feature Selection (Keep only important features)\n",
        "corr_matrix = X.corrwith(y).abs().sort_values(ascending=False)\n",
        "selected_features = corr_matrix[:30].index  # Top 30 most correlated features\n",
        "X = X[selected_features]\n",
        "\n",
        "# Train-Val Split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "\n",
        "# Normalize Features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Reshape for CNN + LSTM\n",
        "X_train_cnn = X_train_scaled.reshape(-1, X_train.shape[1], 1)\n",
        "X_val_cnn = X_val_scaled.reshape(-1, X_val.shape[1], 1)\n",
        "\n",
        "# âœ… Optimized CNN + LSTM Model\n",
        "cnn_lstm_model = Sequential([\n",
        "    Conv1D(filters=256, kernel_size=3, activation='relu', kernel_regularizer=l2(0.005), input_shape=(X_train.shape[1], 1)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Bidirectional(LSTM(128, return_sequences=True)),\n",
        "    Dropout(0.3),\n",
        "    Bidirectional(LSTM(64)),\n",
        "    Dense(128, activation='relu', kernel_regularizer=l2(0.005)),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)  # Regression output\n",
        "])\n",
        "\n",
        "# âœ… Fine-Tuned Learning Rate\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "cnn_lstm_model.compile(optimizer=optimizer, loss=\"mse\")\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=5, min_lr=1e-6),\n",
        "    EarlyStopping(monitor=\"val_loss\", patience=12, restore_best_weights=True)\n",
        "]\n",
        "\n",
        "# Train CNN + LSTM\n",
        "cnn_lstm_model.fit(X_train_cnn, y_train, validation_data=(X_val_cnn, y_val),\n",
        "                   epochs=120, batch_size=128, verbose=1, callbacks=callbacks)\n",
        "\n",
        "# Predictions\n",
        "cnn_lstm_preds_train = cnn_lstm_model.predict(X_train_cnn).flatten()\n",
        "cnn_lstm_preds_val = cnn_lstm_model.predict(X_val_cnn).flatten()\n",
        "\n",
        "# âœ… Improved XGBoost Model\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.012,\n",
        "    max_depth=12,\n",
        "    min_child_weight=3,\n",
        "    colsample_bytree=0.90,\n",
        "    subsample=0.90,\n",
        "    reg_alpha=0.2,\n",
        "    reg_lambda=0.5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# XGBoost Predictions\n",
        "xgb_preds_train = xgb_model.predict(X_train_scaled)\n",
        "xgb_preds_val = xgb_model.predict(X_val_scaled)\n",
        "\n",
        "# âœ… Extra LightGBM Model (To Improve Accuracy)\n",
        "lgb_model = lgb.LGBMRegressor(\n",
        "    n_estimators=1500,\n",
        "    learning_rate=0.009,\n",
        "    max_depth=10,\n",
        "    num_leaves=40,\n",
        "    subsample=0.85,\n",
        "    colsample_bytree=0.85,\n",
        "    reg_alpha=0.2,\n",
        "    reg_lambda=0.3\n",
        ")\n",
        "\n",
        "lgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# LightGBM Predictions\n",
        "lgb_preds_train = lgb_model.predict(X_train_scaled)\n",
        "lgb_preds_val = lgb_model.predict(X_val_scaled)\n",
        "\n",
        "# âœ… Extra Stacking Layer (Better Ensemble)\n",
        "stacked_train = np.column_stack((cnn_lstm_preds_train, xgb_preds_train, lgb_preds_train))\n",
        "stacked_val = np.column_stack((cnn_lstm_preds_val, xgb_preds_val, lgb_preds_val))\n",
        "\n",
        "# Meta-Model (Final Blend)\n",
        "meta_model = lgb.LGBMRegressor(\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.006,\n",
        "    max_depth=7,\n",
        "    num_leaves=25\n",
        ")\n",
        "\n",
        "meta_model.fit(stacked_train, y_train)\n",
        "\n",
        "# Final Predictions\n",
        "final_preds = meta_model.predict(stacked_val)\n",
        "\n",
        "# âœ… Optimized Weighted Ensemble (Fine-Tuned Weights)\n",
        "final_preds = 0.4 * final_preds + 0.3 * cnn_lstm_preds_val + 0.2 * xgb_preds_val + 0.1 * lgb_preds_val\n",
        "\n",
        "# âœ… Enhanced Evaluation Metrics\n",
        "mse = mean_squared_error(y_val, final_preds)\n",
        "exact_matches = np.sum(np.round(final_preds) == np.round(y_val))\n",
        "accuracy = exact_matches / len(y_val)\n",
        "\n",
        "# âœ… New Harmonic Score Formula (More Weight on Accuracy)\n",
        "hs = (6 * (1/mse) * accuracy) / ((1/mse) + accuracy)\n",
        "\n",
        "print(f\"ðŸ”¹ MSE: {mse:.4f}, Accuracy: {accuracy:.4f}, Harmonic Score: {hs:.4f}\")\n",
        "\n",
        "# ðŸ”¹ðŸ”¹ðŸ”¹ TEST SUBMISSION ðŸ”¹ðŸ”¹ðŸ”¹\n",
        "test_data = pd.read_csv(\"/content/test.csv\")\n",
        "\n",
        "# Extract ID\n",
        "test_ids = test_data[\"ID\"]\n",
        "\n",
        "# Prepare Test Features\n",
        "X_test = test_data[selected_features]\n",
        "X_test.fillna(X_test.median(), inplace=True)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "X_test_cnn = X_test_scaled.reshape(-1, X_test.shape[1], 1)\n",
        "\n",
        "# Predict using CNN + LSTM, XGBoost, LightGBM\n",
        "cnn_lstm_preds_test = cnn_lstm_model.predict(X_test_cnn).flatten()\n",
        "xgb_preds_test = xgb_model.predict(X_test_scaled)\n",
        "lgb_preds_test = lgb_model.predict(X_test_scaled)\n",
        "\n",
        "# Stacked Predictions for Test Set\n",
        "stacked_test = np.column_stack((cnn_lstm_preds_test, xgb_preds_test, lgb_preds_test))\n",
        "\n",
        "# Meta-Model Final Prediction\n",
        "final_test_preds = meta_model.predict(stacked_test)\n",
        "\n",
        "# Weighted Averaging for Final Test Predictions\n",
        "final_test_preds = 0.4 * final_test_preds + 0.3 * cnn_lstm_preds_test + 0.2 * xgb_preds_test + 0.1 * lgb_preds_test\n",
        "\n",
        "# Create Submission File\n",
        "submission = pd.DataFrame({\"ID\": test_ids, \"score\": final_test_preds})\n",
        "submission.to_csv(\"submission_HS40.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Submission file saved as submission_HS40.csv ðŸŽ¯\")\n"
      ],
      "metadata": {
        "id": "vLE8VeoSYOfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ðŸ”¹ Load the submission file\n",
        "submission = pd.read_csv(\"submission.csv\")\n",
        "\n",
        "# ðŸ”¹ Round the 'score' column to the nearest integer\n",
        "submission[\"score\"] = submission[\"score\"].round().astype(int)\n",
        "\n",
        "# ðŸ”¹ Save it back to the same file\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Rounded scores (integers) updated in submission.csv successfully!\")\n"
      ],
      "metadata": {
        "id": "rkWZPfKnYZ-Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}